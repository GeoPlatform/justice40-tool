DetectChangesForWorker:
  handler: functions/detect-changes-for-worker/index.handler
  name: ${self:provider.stage}-DetectChangesForWorker
  description: Scans an S3 bucket (with prefix) for items that have changes recently and sends them to ECS Tasks for processing
  runtime: nodejs12.x
  memorySize: 512
  timeout: 900
  environment:
    REGION: ${self:provider.region}
    STAGE: ${self:provider.stage}
    ECS_CLUSTER: !Ref ECSCluster
    VPC_SUBNET_ID:
      Fn::ImportValue: ${self:provider.stage}-PrivateSubnetOne
    GDAL_TASK_DEFINITION: ${self:custom.environment.GDAL_TASK_DEFINITION_NAME}
    GDAL_CONTAINER_DEFINITION: ${self:custom.environment.GDAL_CONTAINER_DEFINITION_NAME}
    TIPPECANOE_TASK_DEFINITION: ${self:custom.environment.TIPPECANOE_TASK_DEFINITION_NAME}
    TIPPECANOE_CONTAINER_DEFINITION: ${self:custom.environment.TIPPECANOE_CONTAINER_DEFINITION_NAME}

  # There are two phases that are processed.
  #
  #  1. Looking for new/updated ZIP files that should be joined to the USDS scoring data into GeoJSON files
  #  2. Looking for new/updated GeoJSON files that should be rendered into tiles
  #
  # The `input` values are attached directly to the Lambda event object
  events:
    - schedule:
        rate: cron(0 5 * * ? *) # Scan for updated data at Midnight Eastern Time
        input:
          action: enrichment
          sourceBucketName: !Ref DataBucket
          sourceBucketPrefix: sources
          age: 86400 # Seconds
          sql:
            Fn::Join: ['', ["SELECT * FROM $", "{s3.Key:noext} LEFT JOIN '/home/custom.csv'.custom ON tabblock2010_01_pophu.BLOCKID10 = custom.BLOCKID10"]]
          destBucketName: !Ref DataBucket
          destBucketPrefix: joined
    - schedule:
        rate: cron(0 7 * * ? *) # Run two hours after the generating any GeoJSON
        input:
          action: none
          sourceBucketName: !Ref DataBucket
          sourceBucketPrefix: joined
          age: 86400 # Seconds
          destBucketName: !Ref DataBucket
          destBucketPrefix: tiles
